{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simple vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules \n",
    "from langchain.docstore.document import Document\n",
    "import os \n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.llms import HuggingFaceHub\n",
    "# from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_document_text = \"\"\" \n",
    "Best Practices for Graph Structure and State\n",
    "\n",
    "When building LangGraph workflows, keep these best practices in mind:\n",
    "\n",
    "    Design a clear State schema: Define your state’s structure up front (using a TypedDict, Pydantic model, or dataclass) to list all fields that nodes will use. This makes it clear what data flows through the graph and helps avoid key collisions or missing data.\n",
    "\n",
    "    Return state updates from nodes: Each node function should return a dictionary of updates to the state. Only the keys returned will be updated or added to the shared state​\n",
    "    stackoverflow.com\n",
    "    . For example, if your state has a field \"progress_step\", and a node updates it, return {\"progress_step\": new_value} from that node​\n",
    "    stackoverflow.com\n",
    "    . This ensures the intended data is persisted in the global state. (If a node returns END or a string instead of a dict, LangGraph interprets that as a routing instruction rather than a state update.)\n",
    "\n",
    "    Name nodes and state fields descriptively: Use meaningful names for node keys (e.g., \"llm\" for a language model step, \"summarize_tool\" for a summarization tool) and for state fields (e.g., \"query\", \"answer\", \"analysis\"). This makes the graph easier to read and maintain. Some developers suffix node names with _node and routing functions with _router (or similar) to avoid confusion between nodes versus routing logic​\n",
    "    stackoverflow.com\n",
    "    .\n",
    "\n",
    "    Set an entry point and a termination: Always specify an entry point so it’s clear where execution begins. Likewise, ensure your graph can terminate. For linear flows or simple one-step graphs, if a node has no outgoing edges, the execution will end after that node. In loops or conditional flows, use END (or workflow.set_finish_point(node)) for at least one branch so the agent can stop. This prevents infinite loops and clearly defines an endpoint​\n",
    "    blog.langchain.dev\n",
    "    .\n",
    "\n",
    "    Keep state updates independent if possible: Each node should ideally produce its piece of state without requiring internal knowledge of how other nodes store data (beyond the agreed-upon state schema). This modular approach makes it easier to add or swap out nodes without breaking the graph’s state logic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(simple_document_text, metadata={\"title\": \"Langgraph Best Practices\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_models():\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    assert OPENAI_API_KEY, \"Please set OPENAI_API_KEY environment variable\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    return llm, embeddings\n",
    "\n",
    "def get_opensource_models():\n",
    "    HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    assert HUGGINGFACEHUB_API_TOKEN, \"Please set HUGGINGFACEHUB_API_TOKEN environment variable\"\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    "        model_kwargs={\"temperature\": 0.2}\n",
    "    )\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    return llm, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, embeddings = get_openai_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Langgraph Best Practices', 'start_index': 2}, page_content='Best Practices for Graph Structure and State\\n\\nWhen building LangGraph workflows, keep these best practices in mind:\\n\\n    Design a clear State schema: Define your state’s structure up front (using a TypedDict, Pydantic model, or dataclass) to list all fields that nodes will use. This makes it clear what data flows through the graph and helps avoid key collisions or missing data.\\n\\n    Return state updates from nodes: Each node function should return a dictionary of updates to the state. Only the keys returned will be updated or added to the shared state\\u200b\\n    stackoverflow.com\\n    . For example, if your state has a field \"progress_step\", and a node updates it, return {\"progress_step\": new_value} from that node\\u200b\\n    stackoverflow.com\\n    . This ensures the intended data is persisted in the global state. (If a node returns END or a string instead of a dict, LangGraph interprets that as a routing instruction rather than a state update.)'),\n",
       " Document(metadata={'title': 'Langgraph Best Practices', 'start_index': 952}, page_content='Name nodes and state fields descriptively: Use meaningful names for node keys (e.g., \"llm\" for a language model step, \"summarize_tool\" for a summarization tool) and for state fields (e.g., \"query\", \"answer\", \"analysis\"). This makes the graph easier to read and maintain. Some developers suffix node names with _node and routing functions with _router (or similar) to avoid confusion between nodes versus routing logic\\u200b\\n    stackoverflow.com\\n    .\\n\\n    Set an entry point and a termination: Always specify an entry point so it’s clear where execution begins. Likewise, ensure your graph can terminate. For linear flows or simple one-step graphs, if a node has no outgoing edges, the execution will end after that node. In loops or conditional flows, use END (or workflow.set_finish_point(node)) for at least one branch so the agent can stop. This prevents infinite loops and clearly defines an endpoint\\u200b\\n    blog.langchain.dev\\n    .'),\n",
       " Document(metadata={'title': 'Langgraph Best Practices', 'start_index': 1889}, page_content='Keep state updates independent if possible: Each node should ideally produce its piece of state without requiring internal knowledge of how other nodes store data (beyond the agreed-upon state schema). This modular approach makes it easier to add or swap out nodes without breaking the graph’s state logic.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_doc = text_splitter.split_documents([doc])\n",
    "chunked_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name='openai_embeddings',\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory='./openai_chroma_langchain_db',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['762e4b68-8094-4477-ac8f-ed84d2c9db04',\n",
       " '81fec0b6-cf87-4f80-b003-015fd4c8d0a8',\n",
       " '1a289fa9-0850-438d-b536-cb31673d67b3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuids = [str(uuid4()) for _ in range(len(chunked_doc))]\n",
    "\n",
    "vector_store.add_documents(documents=chunked_doc, ids=uuids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_ASKiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
